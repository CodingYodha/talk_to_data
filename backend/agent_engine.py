import json
import re
from router import determine_complexity
from database_utils import DBManager
from prompts import build_system_prompt
from llm_client import call_llm, call_llm_raw


def _fallback_to_pro(db: DBManager, user_question: str, full_prompt: str, 
                     previous_error: str = None, previous_sql: str = None) -> tuple:
    """
    Fallback to PRO (Sonnet) model when Flash (Haiku) fails.
    Always uses 'pro' model regardless of question complexity.
    
    Returns:
        tuple: (step_info dict, final_data list or None)
    """
    print("[FALLBACK] Switching to PRO (Sonnet) model...")
    
    if previous_error and previous_sql:
        retry_prompt = f"""
        The previous query failed with error: {previous_error}
        The query was: {previous_sql}
        
        Please correct the SQL query to answer the original question: "{user_question}"
        
        Ensure you return the response in this STRICT JSON format:
        {{
           "thought_process": "Reasoning for the fix...",
           "sql_query": "Corrected SQL..."
        }}
        """
    else:
        retry_prompt = full_prompt
    
    step_info = {"attempt": 2, "thought": None, "sql": None, "error": None}
    
    try:
        raw_response = call_llm(retry_prompt, model_type='pro')
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if step_info["sql"]:
            df, error = db.execute_query(step_info["sql"])
            if error:
                step_info["error"] = error
                return step_info, None
            else:
                return step_info, df.to_dict(orient='records')
        else:
            step_info["error"] = "No SQL generated by PRO model"
            return step_info, None
            
    except json.JSONDecodeError:
        step_info["error"] = "Failed to parse PRO model response as JSON"
        return step_info, None
    except Exception as e:
        step_info["error"] = f"PRO model error: {str(e)}"
        return step_info, None


def _generate_suggestions(user_question: str, sql_code: str, data_sample: list) -> list:
    """
    Generate 3 follow-up question suggestions based on the query and data.
    Uses flash model for speed.
    
    Returns:
        list: 3 suggested follow-up questions
    """
    if not data_sample:
        return []
    
    # Take first 3 rows for context
    sample_str = json.dumps(data_sample[:3], indent=2)
    
    prompt = f"""
    Based on this SQL query and data, suggest exactly 3 short follow-up questions the user might ask.
    
    User's Question: {user_question}
    SQL Query: {sql_code}
    Sample Data: {sample_str}
    
    Return ONLY a JSON array of 3 strings, like:
    ["Question 1?", "Question 2?", "Question 3?"]
    
    Keep questions short (under 10 words each). Focus on filtering, sorting, or drilling down.
    """
    
    try:
        raw_response = call_llm_raw(prompt, model_type='flash')
        # Try to extract JSON array from response
        array_match = re.search(r'\[.*?\]', raw_response, re.DOTALL)
        if array_match:
            suggestions = json.loads(array_match.group(0))
            if isinstance(suggestions, list) and len(suggestions) >= 3:
                return [str(s) for s in suggestions[:3]]
        return []
    except Exception as e:
        print(f"[WARNING] Failed to generate suggestions: {e}")
        return []


def _generate_data_summary(user_question: str, data_sample: list) -> str:
    """
    Generate a 1-sentence business insight summarizing the data.
    Uses flash model for speed.
    
    Returns:
        str: 1-sentence data summary
    """
    if not data_sample:
        return ""
    
    # Take first 5 rows
    sample_str = json.dumps(data_sample[:5], indent=2)
    
    prompt = f"""
    User asked: "{user_question}"
    Data found (first 5 rows): {sample_str}
    
    Summarize the key insight or trend in exactly 1 sentence. Be specific with numbers if available.
    Return ONLY the summary sentence, no JSON, no quotes, no markdown.
    """
    
    try:
        raw_response = call_llm_raw(prompt, model_type='flash')
        # Clean up the response - remove any quotes, markdown, or extra text
        summary = raw_response.strip()
        summary = summary.strip('"').strip("'")
        summary = re.sub(r'^[`\'"]+|[`\'"]+$', '', summary)  # Remove backticks and quotes
        # If it starts with JSON-like content, it failed
        if summary.startswith('{') or summary.startswith('['):
            return ""
        return summary
    except Exception as e:
        print(f"[WARNING] Failed to generate data summary: {e}")
        return ""


def process_question(user_question: str, previous_sql: str = None) -> dict:
    """
    Orchestrates the Text-to-SQL logic with robust fallback:
    1. Determine complexity (Router).
    2. Get schema context (DB).
    3. Build System Prompt (with previous SQL context if provided).
    4. Call LLM (Flash/Haiku first).
    5. Execute SQL.
    6. If ANY failure occurs, ALWAYS fallback to PRO (Sonnet).
    7. On success: Generate suggestions and data summary.
    
    Args:
        user_question (str): The user's natural language question.
        previous_sql (str, optional): Previously executed SQL for context.
        
    Returns:
        dict: Contains question, steps, final_data, suggestions, data_summary, status.
    """
    response_structure = {
        "question": user_question,
        "model_used": None,
        "steps": [],
        "final_data": None,
        "suggestions": [],
        "data_summary": "",
        "status": "pending"
    }

    # 1. Router
    complexity = determine_complexity(user_question)
    response_structure["model_used"] = complexity
    
    # 2. Database Context
    db = DBManager()
    schema_summary = db.get_schema_summary()
    
    # 3. Prompt Construction (with previous SQL context if provided)
    system_prompt = build_system_prompt(schema_summary)
    
    if previous_sql:
        full_prompt = f"""{system_prompt}

### Previous Context
The user's previous query generated this SQL:
```sql
{previous_sql}
```

If the new question is a follow-up (e.g., "filter by...", "sort by...", "show only..."), 
modify the previous SQL accordingly. Otherwise, generate a fresh query.

User Question: {user_question}"""
    else:
        full_prompt = f"{system_prompt}\n\nUser Question: {user_question}"
    
    # 4. LLM Call (Attempt 1)
    step_info = {"attempt": 1, "thought": None, "sql": None, "error": None}
    needs_fallback = False
    fallback_error = None
    fallback_sql = None
    
    try:
        print(f"[ATTEMPT 1] Using {complexity.upper()} model...")
        raw_response = call_llm(full_prompt, model_type=complexity)
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if not step_info["sql"]:
            step_info["error"] = "No SQL generated by LLM"
            needs_fallback = True
            print("[ATTEMPT 1] No SQL generated - triggering fallback")
        else:
            # 5. Execute SQL
            df, error = db.execute_query(step_info["sql"])
            
            if error:
                step_info["error"] = error
                fallback_error = error
                fallback_sql = step_info["sql"]
                needs_fallback = True
                print(f"[ATTEMPT 1] SQL execution failed: {error} - triggering fallback")
            else:
                # Success on Attempt 1!
                final_data = df.to_dict(orient='records')
                response_structure["final_data"] = final_data
                response_structure["status"] = "success"
                response_structure["steps"].append(step_info)
                
                # Generate suggestions and summary
                print("[SUCCESS] Generating suggestions and summary...")
                response_structure["suggestions"] = _generate_suggestions(
                    user_question, step_info["sql"], final_data
                )
                response_structure["data_summary"] = _generate_data_summary(
                    user_question, final_data
                )
                
                return response_structure
                
    except json.JSONDecodeError as e:
        step_info["error"] = f"Failed to parse LLM response as JSON: {str(e)}"
        needs_fallback = True
        print("[ATTEMPT 1] JSON parse error - triggering fallback")
    except Exception as e:
        step_info["error"] = f"Unexpected error: {str(e)}"
        needs_fallback = True
        print(f"[ATTEMPT 1] Exception: {e} - triggering fallback")
    
    response_structure["steps"].append(step_info)
    
    # 6. FALLBACK to PRO (Sonnet)
    if needs_fallback:
        response_structure["model_used"] = "pro (fallback)"
        
        retry_step, final_data = _fallback_to_pro(
            db=db,
            user_question=user_question,
            full_prompt=full_prompt,
            previous_error=fallback_error,
            previous_sql=fallback_sql
        )
        
        response_structure["steps"].append(retry_step)
        
        if final_data:
            response_structure["final_data"] = final_data
            response_structure["status"] = "success"
            
            # Generate suggestions and summary on fallback success too
            print("[FALLBACK SUCCESS] Generating suggestions and summary...")
            response_structure["suggestions"] = _generate_suggestions(
                user_question, retry_step["sql"], final_data
            )
            response_structure["data_summary"] = _generate_data_summary(
                user_question, final_data
            )
        else:
            response_structure["status"] = "error"
    
    return response_structure
