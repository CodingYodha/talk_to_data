import json
import re
import asyncio
from router import determine_complexity
from database_utils import DBManager
from prompts import build_system_prompt
from llm_client import call_llm, call_llm_raw, call_llm_raw_async


def _fallback_to_pro(db: DBManager, user_question: str, full_prompt: str, 
                     previous_error: str = None, previous_sql: str = None) -> tuple:
    """
    Fallback to PRO (Sonnet) model when Flash (Haiku) fails.
    """
    print("[FALLBACK] Switching to PRO (Sonnet) model...")
    
    if previous_error and previous_sql:
        retry_prompt = f"""
        The previous query failed with error: {previous_error}
        The query was: {previous_sql}
        
        Please correct the SQL query to answer the original question: "{user_question}"
        
        Ensure you return the response in this STRICT JSON format:
        {{
           "thought_process": "Reasoning for the fix...",
           "sql_query": "Corrected SQL..."
        }}
        """
    else:
        retry_prompt = full_prompt
    
    step_info = {"attempt": 2, "thought": None, "sql": None, "error": None}
    
    try:
        raw_response = call_llm(retry_prompt, model_type='pro')
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if step_info["sql"]:
            df, error = db.execute_query(step_info["sql"])
            if error:
                step_info["error"] = error
                return step_info, None
            else:
                return step_info, df.to_dict(orient='records')
        else:
            step_info["error"] = "No SQL generated by PRO model"
            return step_info, None
            
    except json.JSONDecodeError:
        step_info["error"] = "Failed to parse PRO model response as JSON"
        return step_info, None
    except Exception as e:
        step_info["error"] = f"PRO model error: {str(e)}"
        return step_info, None


# ============ ASYNC HELPER FUNCTIONS FOR PARALLEL EXECUTION ============

async def _is_follow_up_query_async(current_question: str, previous_sql: str) -> bool:
    """
    Async: Detect if current question is a follow-up to the previous query.
    """
    if not previous_sql:
        return False
    
    prompt = f"""Previous SQL: {previous_sql[:200]}
Question: {current_question}

Is this a follow-up/refinement of the previous query? Answer ONLY "yes" or "no"."""

    try:
        response = await call_llm_raw_async(prompt, model_type='flash')
        is_followup = 'yes' in response.lower().strip()
        print(f"[ASYNC FOLLOW-UP] LLM says: {response.strip()} -> {is_followup}")
        return is_followup
    except Exception as e:
        print(f"[ASYNC FOLLOW-UP] Failed: {e}")
        return False


async def _generate_suggestions_async(user_question: str, sql_code: str, data_sample: list) -> list:
    """
    Async: Generate 3 follow-up question suggestions.
    """
    if not data_sample:
        return []
    
    sample_str = json.dumps(data_sample[:3], indent=2)
    
    prompt = f"""Based on this SQL query and data, suggest exactly 3 short follow-up questions.
    
User's Question: {user_question}
SQL Query: {sql_code}
Sample Data: {sample_str}

Return ONLY a JSON array: ["Q1?", "Q2?", "Q3?"]
Keep questions under 10 words."""

    try:
        raw_response = await call_llm_raw_async(prompt, model_type='flash')
        array_match = re.search(r'\[.*?\]', raw_response, re.DOTALL)
        if array_match:
            suggestions = json.loads(array_match.group(0))
            if isinstance(suggestions, list) and len(suggestions) >= 3:
                return [str(s) for s in suggestions[:3]]
        return []
    except Exception as e:
        print(f"[ASYNC SUGGESTIONS] Failed: {e}")
        return []


async def _generate_data_summary_async(user_question: str, data_sample: list) -> str:
    """
    Async: Generate a 1-sentence business insight.
    """
    if not data_sample:
        return ""
    
    sample_str = json.dumps(data_sample[:5], indent=2)
    
    prompt = f"""User asked: "{user_question}"
Data found (first 5 rows): {sample_str}

Summarize the key insight in exactly 1 sentence. Be specific with numbers.
Return ONLY the summary sentence, no JSON, no quotes."""

    try:
        raw_response = await call_llm_raw_async(prompt, model_type='flash')
        summary = raw_response.strip().strip('"').strip("'")
        summary = re.sub(r'^[`\'"]+|[`\'"]+$', '', summary)
        if summary.startswith('{') or summary.startswith('['):
            return ""
        return summary
    except Exception as e:
        print(f"[ASYNC SUMMARY] Failed: {e}")
        return ""


async def _run_parallel_post_processing(
    user_question: str, 
    sql_code: str, 
    final_data: list, 
    previous_sql: str
) -> tuple:
    """
    Run follow-up check, suggestions, and summary generation in PARALLEL.
    Returns (is_followup, suggestions, data_summary).
    """
    print("[PARALLEL] Starting 3 async LLM calls...")
    
    # Run all 3 LLM calls simultaneously
    is_followup, suggestions, summary = await asyncio.gather(
        _is_follow_up_query_async(user_question, previous_sql),
        _generate_suggestions_async(user_question, sql_code, final_data),
        _generate_data_summary_async(user_question, final_data)
    )
    
    print(f"[PARALLEL] Complete: followup={is_followup}, suggestions={len(suggestions)}, summary_len={len(summary)}")
    
    # Only return summary if it's a follow-up query
    return is_followup, suggestions, summary if is_followup else ""


def process_question(user_question: str, previous_sql: str = None) -> dict:
    """
    Orchestrates the Text-to-SQL logic with robust fallback.
    Uses PARALLEL LLM calls for post-processing (suggestions, summary, follow-up check).
    """
    response_structure = {
        "question": user_question,
        "model_used": None,
        "steps": [],
        "final_data": None,
        "suggestions": [],
        "data_summary": "",
        "status": "pending"
    }

    # 1. Router
    complexity = determine_complexity(user_question)
    response_structure["model_used"] = complexity
    
    # 2. Database Context
    db = DBManager()
    schema_summary = db.get_schema_summary()
    
    # 3. Prompt Construction
    system_prompt = build_system_prompt(schema_summary)
    
    if previous_sql:
        full_prompt = f"""{system_prompt}

### Previous Context
The user's previous query generated this SQL:
```sql
{previous_sql}
```

If the new question is a follow-up, modify the previous SQL. Otherwise, generate a fresh query.

User Question: {user_question}"""
    else:
        full_prompt = f"{system_prompt}\n\nUser Question: {user_question}"
    
    # 4. LLM Call (Attempt 1)
    step_info = {"attempt": 1, "thought": None, "sql": None, "error": None}
    needs_fallback = False
    fallback_error = None
    fallback_sql = None
    
    try:
        print(f"[ATTEMPT 1] Using {complexity.upper()} model...")
        raw_response = call_llm(full_prompt, model_type=complexity)
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if not step_info["sql"]:
            step_info["error"] = "No SQL generated by LLM"
            needs_fallback = True
            print("[ATTEMPT 1] No SQL generated - triggering fallback")
        else:
            # 5. Execute SQL
            df, error = db.execute_query(step_info["sql"])
            
            if error:
                step_info["error"] = error
                fallback_error = error
                fallback_sql = step_info["sql"]
                needs_fallback = True
                print(f"[ATTEMPT 1] SQL execution failed: {error} - triggering fallback")
            else:
                # Success on Attempt 1!
                final_data = df.to_dict(orient='records')
                response_structure["final_data"] = final_data
                response_structure["status"] = "success"
                response_structure["steps"].append(step_info)
                
                # Run post-processing in PARALLEL
                print("[SUCCESS] Running parallel post-processing...")
                is_followup, suggestions, summary = asyncio.run(
                    _run_parallel_post_processing(
                        user_question, step_info["sql"], final_data, previous_sql
                    )
                )
                
                response_structure["suggestions"] = suggestions
                response_structure["data_summary"] = summary
                
                return response_structure
                
    except json.JSONDecodeError as e:
        step_info["error"] = f"Failed to parse LLM response as JSON: {str(e)}"
        needs_fallback = True
        print("[ATTEMPT 1] JSON parse error - triggering fallback")
    except Exception as e:
        step_info["error"] = f"Unexpected error: {str(e)}"
        needs_fallback = True
        print(f"[ATTEMPT 1] Exception: {e} - triggering fallback")
    
    response_structure["steps"].append(step_info)
    
    # 6. FALLBACK to PRO (Sonnet)
    if needs_fallback:
        response_structure["model_used"] = "pro (fallback)"
        
        retry_step, final_data = _fallback_to_pro(
            db=db,
            user_question=user_question,
            full_prompt=full_prompt,
            previous_error=fallback_error,
            previous_sql=fallback_sql
        )
        
        response_structure["steps"].append(retry_step)
        
        if final_data:
            response_structure["final_data"] = final_data
            response_structure["status"] = "success"
            
            # Run post-processing in PARALLEL on fallback success
            print("[FALLBACK SUCCESS] Running parallel post-processing...")
            is_followup, suggestions, summary = asyncio.run(
                _run_parallel_post_processing(
                    user_question, retry_step["sql"], final_data, previous_sql
                )
            )
            
            response_structure["suggestions"] = suggestions
            response_structure["data_summary"] = summary
        else:
            response_structure["status"] = "error"
    
    return response_structure
